{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95da603f-f0c3-47b8-90c1-9cf1654e1211",
   "metadata": {},
   "outputs": [],
   "source": [
    "import generate_retrieval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f1fe7e-b354-4c35-815e-2795ec06f1c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "merged_nq_table_data_loc = \"/home/deokhk/research/MultiQA/dataset/NQ_tables/interactions/merged_interaction.json\"\n",
    "dpr_nq_train_data_loc = \"/home/deokhk/research/MultiQA/model/DPR/dpr/downloads/data/retriever/nq-train.json\"\n",
    "dpr_nq_dev_data_loc = \"/home/deokhk/research/MultiQA/model/DPR/dpr/downloads/data/retriever/nq-dev.json\"\n",
    "original_nq_train_data_loc = \"/home/deokhk/research/MultiQA/model/DPR/dpr/downloads/data/retriever/qas/nq-train.csv\"\n",
    "original_nq_dev_data_loc = \"/home/deokhk/research/MultiQA/model/DPR/dpr/downloads/data/retriever/qas/nq-dev.csv\"\n",
    "original_nq_test_data_loc = \"/home/deokhk/research/MultiQA/model/DPR/dpr/downloads/data/retriever/qas/nq-test.csv\"\n",
    "\n",
    "table_q_list = generate_retrieval_dataset.extract_q_from_interaction_file_and_save(merged_nq_table_data_loc)\n",
    "filtered_table_q_list = generate_retrieval_dataset.filter_and_save_table_qa_dataset(table_q_list, original_nq_train_data_loc, original_nq_dev_data_loc, merged_nq_table_data_loc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e76e87-65fd-4a7d-8df1-c353d8b6f0bc",
   "metadata": {},
   "source": [
    "## Experiment 1: Filtered interaction에 question이 중복인 녀석들이 존재하는것 같음. 이 녀석들을 어떻게 처리해야되는지 확인해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c48b16db-613c-4131-afb8-a271dc7b8983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 우선, 각 interaction data의 format부터 볼까..\n",
    "import json\n",
    "with open(\"/home/deokhk/research/MultiQA/dataset/NQ_tables/interactions/filtered_interaction.json\", 'r') as f:\n",
    "    filtered_interaction = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc746d91-962a-4a17-8e9a-940a26ebbd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "mi_list_form = []\n",
    "for it_object in filtered_interaction:\n",
    "    mi_list_form.append(it_object[\"questions\"][0][\"originalText\"])\n",
    "print(f\"list form: {len(mi_list_form)}, set form: {len(set(mi_list_form))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75947db-b95f-4d47-99af-0941ba685e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp[\"questions\"][0][\"originalText\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93866730-895a-4d12-9799-e918f4b0b7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_that_appeared = set()\n",
    "for it_object in filtered_interaction:\n",
    "    q = it_object[\"questions\"][0][\"originalText\"].strip()\n",
    "    if q in question_that_appeared:\n",
    "        print(q)\n",
    "    else:\n",
    "        question_that_appeared.add(q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584c7066-3392-4018-bdb9-1514694cfc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_that_appeared = set()\n",
    "for it_object in filtered_interaction:\n",
    "    q = it_object[\"questions\"][0][\"originalText\"].strip()\n",
    "    if q==\"what percent of gdp is spent on education in india\":\n",
    "        print(\"=====================================\")\n",
    "        print(it_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27844af0-442b-4c59-8108-718613d8f5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import extract_table_dataframe_from_interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b08eb4-0b72-4e7e-8044-b2945a7dc952",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicated_obj_list = []\n",
    "for it_object in filtered_interaction:\n",
    "    q = it_object[\"questions\"][0][\"originalText\"].strip()\n",
    "    if q==\"what percent of gdp is spent on education in india\":\n",
    "        duplicated_obj_list.append(it_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7300ff48-2b55-4d27-9824-860d391af0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = extract_table_dataframe_from_interaction(duplicated_obj_list[0])\n",
    "df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e04f27-6965-45e2-8821-038e6e6cdf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = extract_table_dataframe_from_interaction(duplicated_obj_list[1])\n",
    "df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814beb95-dabf-4f04-9b18-62790845f038",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_that_appeared = set()\n",
    "duplicated_questions = []\n",
    "for it_object in filtered_interaction:\n",
    "    q = it_object[\"questions\"][0][\"originalText\"].strip()\n",
    "    if q in question_that_appeared:\n",
    "        duplicated_questions.append(q)\n",
    "    else:\n",
    "        question_that_appeared.add(q)\n",
    "\n",
    "question_count = []\n",
    "for elem in duplicated_questions:\n",
    "    question_count.append(duplicated_questions.count(elem))\n",
    "print(set(question_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5c6ae4-093c-4975-b6aa-a9d462bad72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "# Filtered interaction에 속하는 question이 중복된 QA pair에 대하여, NQ_open에서는 어떻게 처리하고 있을지 확인해보자.\n",
    "nq_open_train_loc = \"/home/deokhk/research/MultiQA/model/DPR/dpr/downloads/data/retriever/qas/nq-train.csv\"\n",
    "nq_open_dev_loc = \"/home/deokhk/research/MultiQA/model/DPR/dpr/downloads/data/retriever/qas/nq-dev.csv\"\n",
    "original_nq_open_dict = {}\n",
    "list_form_length_count = 0\n",
    "with open(nq_open_train_loc, 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=\"\\t\")\n",
    "    for row in reader:\n",
    "        question = row[0]\n",
    "        answer = row[1]\n",
    "        original_nq_open_dict[question.strip()] = answer\n",
    "        list_form_length_count+=1\n",
    "\n",
    "with open(nq_open_dev_loc, 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=\"\\t\")\n",
    "    for row in reader:\n",
    "        question = row[0]\n",
    "        original_nq_open_dict[question.strip()] = answer\n",
    "        list_form_length_count+=1\n",
    "\n",
    "if(len(original_nq_open_dict) == list_form_length_count):\n",
    "    print(\"There are no duplicated question in NQ_open!\")\n",
    "else:\n",
    "    print(f\"There is duplication. dictionary form: {len(original_nq_open_dict)}, list form: {list_form_length_count}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670d8c59-381e-45dc-9b6b-b7388265a1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtered interaction에 속하는 question이 중복된 QA pair에 대하여, NQ_open에서는 어떻게 처리하고 있을지 확인해보자.\n",
    "# Filtered interaction에 속하는 중복된 QA pair들에 대하여, 각 중복 QA pair들을 [(q_1, a_1), (q_1_dup, a_2)] 이런 형식으로 표현하자. 아 이러면 안될듯. {\"q1\":[a_1, a_2]} 이런식으로 해야겠는데? \n",
    "# 동일한 q_1에 대하여, NQ_open에서 [(q_1, a_1)] 형태로 표현하자.\n",
    "duplicated_qa_dict ={}\n",
    "# filtered interaction에 속하는 중복된 QA pair에 대하여 검사함.\n",
    "for it_object in filtered_interaction:\n",
    "    q = it_object[\"questions\"][0][\"originalText\"].strip()\n",
    "    if q in duplicated_questions:\n",
    "        if q in duplicated_qa_dict:\n",
    "            answer = it_object[\"questions\"][0][\"answer\"][\"answerTexts\"]\n",
    "            duplicated_qa_dict[q] = duplicated_qa_dict[q] + answer\n",
    "        else:\n",
    "            duplicated_qa_dict[q] = it_object[\"questions\"][0][\"answer\"][\"answerTexts\"]\n",
    "\n",
    "# NQ_open에 속하는 애들..\n",
    "duplicated_nq_open_dict = {}\n",
    "with open(nq_open_train_loc, 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=\"\\t\")\n",
    "    for row in reader:\n",
    "        question = row[0].strip()\n",
    "        if question in duplicated_questions:\n",
    "            answer = row[1]\n",
    "            duplicated_nq_open_dict[question] = answer\n",
    "\n",
    "with open(nq_open_dev_loc, 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=\"\\t\")\n",
    "    for row in reader:\n",
    "        question = row[0].strip()\n",
    "        if question in duplicated_questions:\n",
    "            answer = row[1]\n",
    "            duplicated_nq_open_dict[question] = answer\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d702bd75-4b5c-4604-b2da-bfc24642d58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for q in duplicated_qa_dict:\n",
    "    print(\"==== In filtered interaction ====\")\n",
    "    print(f\"{q} : {duplicated_qa_dict[q]}\")\n",
    "    print(\"==== In NQ_open ==== \")\n",
    "    print(f\"{q} : {duplicated_nq_open_dict[q]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad2689a-ee19-4d9b-8551-17ad694da3db",
   "metadata": {},
   "source": [
    "## DPR training시 question이 Table qa pair와 overllap이 있는 경우, positive passage가 어떠한 형식으로 주어지는지 확인하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776d3996-501e-4738-bb10-c8e6df5a5f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtred interaction과 overlap되는 pair에 대하여, \n",
    "# Filtered interaction + 그리고 overllaped pair 하나를 표시하자.\n",
    "dpr_nq_train_data_loc = \"/home/deokhk/research/MultiQA/model/DPR/dpr/downloads/data/retriever/nq-train.json\"\n",
    "dpr_nq_dev_data_loc = \"/home/deokhk/research/MultiQA/model/DPR/dpr/downloads/data/retriever/nq-dev.json\"\n",
    "\n",
    "with open(dpr_nq_train_data_loc, 'r') as dpr_file:\n",
    "    dpr_data = json.load(dpr_file)\n",
    "    \n",
    "count = 0\n",
    "pos = int(input(\"Which position?\"))\n",
    "for data in dpr_data:\n",
    "    q = data[\"question\"].strip()\n",
    "    for it_object in filtered_interaction:\n",
    "        ft_q = it_object[\"questions\"][0][\"originalText\"].strip()\n",
    "        if q == ft_q:\n",
    "            print(\"==== In DPR ====\")\n",
    "            print(data)\n",
    "            print(\"==== In Table dataset ====\")\n",
    "            print(it_object)\n",
    "            count +=1\n",
    "            break\n",
    "    if count == pos:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d0b7e1-c68f-4640-94b4-8180b9f4f1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# original NQ data에서 정답이 table이나 infobox, list위에 있다고 표시된 경우, DPR에서는 어떻게 처리하고있는지, 그리고 filtered interaction에서는 어떻게 처리하고 있는지 각각 표시함.\n",
    "for data in dpr_data:\n",
    "    q = data[\"question\"].strip()\n",
    "    if q == \"who played in the nba finals in 1994\":\n",
    "        print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925784f0-b07f-46fe-ba1f-09edcae23cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = 0\n",
    "for it_object in filtered_interaction:\n",
    "    q = it_object[\"questions\"][0][\"originalText\"].strip()\n",
    "    if q == \"who won the last 20 over world cup\":\n",
    "        df = extract_table_dataframe_from_interaction(it_object)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fbdff2-78ff-4147-ad16-03ee1d9f6e43",
   "metadata": {},
   "source": [
    "## Filtered_interaction에서 DPR에 이미 들어가 있는 QA pair는 제외하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f28161b9-e5b3-4658-aa9f-3b19aa06a8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicated_qa_pair(dpr_train_loc, dpr_dev_loc, filtered_interaction):\n",
    "    with open(dpr_train_loc, 'r') as f:\n",
    "        dpr_train = json.load(f)\n",
    "    with open(dpr_dev_loc, 'r') as f:\n",
    "        dpr_dev = json.load(f)\n",
    "    \n",
    "    dpr_q = set()\n",
    "    for data in dpr_train:\n",
    "        dpr_q.add(data[\"question\"].strip())\n",
    "    for data in dpr_dev:\n",
    "        dpr_q.add(data[\"question\"].strip())\n",
    "    \n",
    "    duplicated_removed_interaction = []\n",
    "    print(f\"Number of QA pair before duplication removed: {len(filtered_interaction)}\")\n",
    "    for it_object in filtered_interaction:\n",
    "        q = it_object[\"questions\"][0][\"originalText\"].strip()\n",
    "        if q not in dpr_q:\n",
    "            duplicated_removed_interaction.append(it_object)\n",
    "    print(f\"Number of QA pair after duplication removed: {len(duplicated_removed_interaction)}\")\n",
    "    with open(\"/home/deokhk/research/MultiQA/dataset/NQ_tables/interactions/dup_removed_interaction.json\", \"w+\") as f:\n",
    "        json.dump(duplicated_removed_interaction, f)\n",
    "    print(\"Saving of duplicated_removed_interaction completed\")\n",
    "\n",
    "    return duplicated_removed_interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe5b7fa9-ed39-4348-8737-4521da5895ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of QA pair before duplication removed: 8822\n",
      "Number of QA pair after duplication removed: 5137\n",
      "Saving of duplicated_removed_interaction completed\n"
     ]
    }
   ],
   "source": [
    "dpr_train_loc = \"/home/deokhk/research/MultiQA/model/DPR/dpr/downloads/data/retriever/nq-train.json\"\n",
    "dpr_dev_loc = \"/home/deokhk/research/MultiQA/model/DPR/dpr/downloads/data/retriever/nq-dev.json\"\n",
    "dup_removed_interaction = remove_duplicated_qa_pair(dpr_train_loc, dpr_dev_loc, filtered_interaction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e4f79f-8871-4e2f-b6bc-6aef49a1bca5",
   "metadata": {},
   "source": [
    "## Bert tokenizer demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8234c6e-4296-4b98-b9d9-f2471796447c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-cased\",additional_special_tokens=['[C_SEP]', '[R_SEP]'])\n",
    "sent = \"I have a new GPU![CLS] hey\"\n",
    "print(sent)\n",
    "r1= tokenizer.tokenize(sent)\n",
    "reconstructed = tokenizer.decode(tokenizer.convert_tokens_to_ids(r1))\n",
    "print(reconstructed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0feb89-13a1-4d33-b054-3176fa8057cc",
   "metadata": {},
   "source": [
    "## Table passage generate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff71089c-c333-4b0c-976b-b1b07fb8f11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "import json\n",
    "import csv\n",
    "from time import sleep\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def linearize_column(column_list):\n",
    "    linearized_column =\"\"\n",
    "    cl = len(column_list)\n",
    "    for i, column in enumerate(column_list):\n",
    "        linearized_column +=column\n",
    "        if i != cl-1:\n",
    "            linearized_column+=\" \"\n",
    "    return linearized_column\n",
    "        \n",
    "    \n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\", additional_special_tokens = ['[C_SEP]', '[R_SEP]'])\n",
    "table_data_loc = \"/home/deokhk/research/MultiQA/dataset/NQ_tables/tables/tables.jsonl\"\n",
    "with open(table_data_loc, 'r') as table_file:\n",
    "    table_datas = list(table_file)\n",
    "\n",
    "count = 0\n",
    "psg_count = 21015325 # Since the last number of wikipedia split is 21015324\n",
    "\n",
    "f = open(\"/home/deokhk/research/MultiQA/model/DPR/dpr/downloads/data/wikipedia_split/table_w100.tsv\", \"wt\")\n",
    "tsv_writer = csv.writer(f, delimiter='\\t')\n",
    "\n",
    "for table in table_datas:\n",
    "    print(table)\n",
    "    table = json.loads(table)\n",
    "    title = table[\"documentTitle\"]\n",
    "    columns =[]\n",
    "    for i, column in enumerate(table[\"columns\"]):\n",
    "        columns.append(column[\"text\"])\n",
    "        if i != len(table[\"columns\"])-1:\n",
    "            columns.append(\"[C_SEP]\")\n",
    "    linearized_column = linearize_column(columns)\n",
    "\n",
    "    linearized_value = \"\"\n",
    "    rows = table[\"rows\"]\n",
    "    for i, row in enumerate(rows):\n",
    "        row_value = row[\"cells\"]\n",
    "        for j, value in enumerate(row_value):\n",
    "            linearized_value += value[\"text\"] \n",
    "            if j != len(row_value)-1:\n",
    "                linearized_value += \"[R_SEP]\"\n",
    "        if i != len(rows)-1:\n",
    "            # Since \"\\n\" is considered as whitespace, we first set delimter for each sentence as \"[CLS]\"\n",
    "            # And replace it to \"\\n\" afterward.\n",
    "            linearized_value += \"[CLS]\"\n",
    "    linearized_value = tokenizer.tokenize(linearized_value)\n",
    "    \n",
    "    # split the linearized value into passages, each with 100 token.\n",
    "    vlen = len(linearized_value)\n",
    "    quotient = vlen // 100\n",
    "    remainder = vlen % 100\n",
    "    psg_list = []\n",
    "    \n",
    "    if quotient == 0:\n",
    "        psg_list.append(linearized_value[0:remainder])\n",
    "    else:\n",
    "        for i in range(quotient+1):\n",
    "\n",
    "            psg_list.append(linearized_value[i*100:(i+1)*100])\n",
    "            if i == quotient and remainder:\n",
    "                psg_list.append(linearized_value[(i+1)*100:(i+1)*100 + remainder])\n",
    "    \n",
    "    for psg in psg_list:\n",
    "        psg = tokenizer.convert_tokens_to_string(psg)\n",
    "        psg = psg.replace(\"[CLS]\", \"\\n\")\n",
    "        tsv_writer.writerow([psg_count, linearized_column + \"[SEP]\" + psg, title])\n",
    "        psg_count+=1\n",
    "    \n",
    "        \n",
    "    count+=1\n",
    "    if count == 2:\n",
    "        break\n",
    "f.close()\n",
    "    #print(len(linearized_value))\n",
    "    #count+=1\n",
    "    #if count==20:\n",
    "        #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e34fa3a-a190-47be-b014-345f18d159b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return splitted table passages.\n",
    "# Form: tsv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8014aed-a298-45ca-8af5-dfe874de5e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "linearized_value = list(range(25))\n",
    "quotient = len(linearized_value) // 10\n",
    "remainder = len(linearized_value) % 10\n",
    "splitted_list = []\n",
    "if quotient == 0:\n",
    "    splitted_list.append(linearized_value[0:remainder])\n",
    "else:\n",
    "    for i in range(quotient):\n",
    "        splitted_list.append(linearized_value[i*10:(i+1)*10])\n",
    "        if i == quotient-1 and remainder:\n",
    "            splitted_list.append(linearized_value[(i+1)*10: (i+1)*10 + remainder])\n",
    "\n",
    "splitted_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef13ec42-739e-41d6-b274-c42997371960",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"Hey [CLS] me\"\n",
    "a.replace(\"[CLS]\", \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540da0fa-3597-48b5-9e3a-baec50799a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\", additional_special_tokens = ['[C_SEP]', '[R_SEP]'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4956a4-b6ec-4995-9749-2e76155cbe54",
   "metadata": {},
   "source": [
    "## Generate table retrieval data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd00c107-a97c-435d-930f-b0e0ede6b36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from transformers import BertTokenizerFast\n",
    "from generate_table_passages import gen_table_passages\n",
    "from tqdm import tqdm\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-cased\", additional_special_tokens = ['[C_SEP]', '[V_SEP]', '[R_SEP]'])\n",
    "\n",
    "tb_retrieval_data = []\n",
    "count = 0\n",
    "for it_object in tqdm(dup_removed_interaction):\n",
    "    data = {}\n",
    "    table = it_object[\"table\"]\n",
    "    title = table[\"documentTitle\"]\n",
    "    question = it_object[\"questions\"][0][\"originalText\"]\n",
    "    answers = it_object[\"questions\"][0][\"answer\"][\"answerTexts\"]\n",
    "    \n",
    "    data[\"question\"] = question\n",
    "    data[\"answers\"] = answers\n",
    "    \n",
    "    positive_context = {}\n",
    "    linearized_column, psg_list = gen_table_passages(table, tokenizer)\n",
    "    gold_psg = \"\"\n",
    "    max_answer_num = 0\n",
    "    for psg in psg_list:\n",
    "        psg = tokenizer.decode(tokenizer.convert_tokens_to_ids(psg))\n",
    "        num_answers_found = 0\n",
    "        for answer in answers:\n",
    "            if answer in psg:\n",
    "                num_answers_found+=1\n",
    "        if num_answers_found > max_answer_num:\n",
    "            max_answer_num = num_answers_found\n",
    "            gold_psg = psg\n",
    "    \n",
    "    gold_text = linearized_column + \" [SEP] \" + gold_psg\n",
    "    positive_context[\"title\"] = title\n",
    "    positive_context[\"text\"] = gold_text\n",
    "    data[\"positive_ctxs\"] = [positive_context]\n",
    "    tb_retrieval_data.append(data)\n",
    "    \n",
    "dlen = len(tb_retrieval_data)\n",
    "train_data = tb_retrieval_data[0:(dlen//10)*9]\n",
    "dev_data = tb_retrieval_data[(dlen//10)*9:]\n",
    "with open(\"/home/deokhk/research/MultiQA/model/DPR/dpr/downloads/data/retriever/table_train.json\", \"w\") as f:\n",
    "    json.dump(train_data, f)\n",
    "with open(\"/home/deokhk/research/MultiQA/model/DPR/dpr/downloads/data/retriever/table_dev.json\", \"w\") as f:\n",
    "    json.dump(dev_data, f)\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925f53cd-c1c5-4d9f-8892-c2dd5f376876",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in save:\n",
    "    print(key)\n",
    "print(\"================================\")\n",
    "for key in save[\"table\"]:\n",
    "    print(key)\n",
    "print(\"================================\")\n",
    "for key in save[\"questions\"]:\n",
    "    print(key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de932d61-b70d-4315-a5be-2cf7ac4eaa61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question\n",
      "answers\n",
      "positive_ctxs\n"
     ]
    }
   ],
   "source": [
    "with open(\"/home/deokhk/research/MultiQA/model/DPR/dpr/downloads/data/retriever/table_train.json\", \"r\") as f:\n",
    "    table_train = json.load(f)\n",
    "\n",
    "for elem in table_train:\n",
    "    for key in elem:\n",
    "        print(key)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2ce35d-2153-478f-8072-a4fd4323d505",
   "metadata": {},
   "source": [
    "## Merge table + text\n",
    "return: nq-train-with-table.json , nq-dev-with-table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6367a8-94db-480d-8f51-9ec95333b63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "files=[']\n",
    "\n",
    "def merge_JsonFiles(filename):\n",
    "    result = list()\n",
    "    for f1 in filename:\n",
    "        with open(f1, 'r') as infile:\n",
    "            result.extend(json.load(infile))\n",
    "\n",
    "    with open('counseling3.json', 'w') as output_file:\n",
    "        json.dump(result, output_file)\n",
    "\n",
    "merge_JsonFiles(files)\n",
    "\n",
    "       /home/deokhk/research/MultiQA/model/DPR/dpr/downloads/data/retriever/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ded9644-5a48-4535-8b3e-085a398c263c",
   "metadata": {},
   "source": [
    "/home/deokhk/research/MultiQA/model/DPR/dpr/downloads/data/retriever/nq-train.json\n",
    "\n",
    "/home/deokhk/research/MultiQA/model/DPR/dpr/downloads/data/retriever/table_train_with_hn.json\n",
    "\n",
    "/home/deokhk/research/MultiQA/model/DPR/dpr/downloads/data/retriever/nq-train-with-table.json\n",
    "\n",
    "/home/deokhk/research/MultiQA/model/DPR/dpr/downloads/data/retriever/nq-dev.json\n",
    "\n",
    "/home/deokhk/research/MultiQA/model/DPR/dpr/downloads/data/retriever/table_dev_with_hn.json\n",
    "\n",
    "/home/deokhk/research/MultiQA/model/DPR/dpr/downloads/data/retriever/nq-dev-with-table.json\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9307af25-2a52-40fe-ac26-f8c9ee8f1902",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"/home/deokhk/research/MultiQA/model/DPR/dpr/downloads/data/retriever/nq-train-with-table.json\", \"r\") as f:\n",
    "     ttab = json.load(f)\n",
    "print(f\"Num of elem in nq-train-with-table: {len(ttab)}\")\n",
    "\n",
    "with open(\"/home/deokhk/research/MultiQA/model/DPR/dpr/downloads/data/retriever/nq-dev-with-table.json\", \"r\") as f:\n",
    "     dtab = json.load(f)\n",
    "print(f\"Num of elem in nq-dev-with-table: {len(dtab)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4639b0e3-1e70-4f62-a59d-e0cee6c90d20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
